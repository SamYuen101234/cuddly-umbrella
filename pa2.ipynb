{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class imageDataset(Dataset):\n",
    "    def __init__(self, df_train, root_dir, transform=None):\n",
    "        self.landmarks_frame = df_train['imdbId'].astype(str) + '.jpg'\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.df_train = df_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_name = os.path.join(self.root_dir, self.landmarks_frame.iloc[idx])\n",
    "        image = Image.open(img_name, 'r').convert('RGB')\n",
    "        #genre = self.df_train.loc[idx,'genre']\n",
    "        genre = np.array([0,0,0,0,0,0,0])\n",
    "        for i in range(7):\n",
    "            if(self.df_train.iloc[idx,i+1] == 1):\n",
    "                genre[i] = 1\n",
    "        genre = torch.from_numpy(genre)\n",
    "        genre = genre.float()\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        #result = {'image': image, 'genre': genre}\n",
    "        return image, genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = transforms.Compose([#transforms.Resize(60),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5 ), (0.5, 0.5, 0.5))])\n",
    "transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./pa2_data/part1_data/train.csv', sep=',')\n",
    "df_train = df_train.drop_duplicates()\n",
    "#df_train.set_index('imdbId',inplace=True)\n",
    "#df_train = df_train[df_train==1].stack().reset_index().drop(0,1)\n",
    "#df_train.columns = ['imdbId', 'genre']\n",
    "train_set = imageDataset(df_train, root_dir = './pa2_data/part1_data/images/', transform=transform)\n",
    "\n",
    "df_test = pd.read_csv('./pa2_data/part1_data/test.csv', sep=',')\n",
    "test_set = imageDataset(df_test, root_dir = './pa2_data/part1_data/images/', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for i in range(10):\\n    image, genre= train_set[i]\\n    temp = np.array(image)\\n    print(train_set.landmarks_frame[y])\\n    print(image.shape)\\n    print(genre)\\n    imshow((torchvision.utils.make_grid(image)))\\n    y = y + 1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(len(dataset))\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "y = 0\n",
    "'''for i in range(10):\n",
    "    image, genre= train_set[i]\n",
    "    temp = np.array(image)\n",
    "    print(train_set.landmarks_frame[y])\n",
    "    print(image.shape)\n",
    "    print(genre)\n",
    "    imshow((torchvision.utils.make_grid(image)))\n",
    "    y = y + 1'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "train_size = int(0.8 * len(train_set))\n",
    "valid_size = len(train_set) - train_size\n",
    "full_train, validation = random_split(train_set, [train_size, valid_size])\n",
    "\n",
    "train_loader = DataLoader(full_train, batch_size=1,\n",
    "                          shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(validation, batch_size=1,\n",
    "                          shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_set, batch_size=1,\n",
    "                         shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "'''class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=25)  \n",
    "        # input channels = 3, 因為RGB所有3個channels, output為6個channels\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16,  kernel_size=17)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=40,  kernel_size=9)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv4 = nn.Conv2d(in_channels=40, out_channels=100,  kernel_size=5)\n",
    "        self.fc1 = nn.Linear(100 * 9 * 3, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 7)\n",
    "        #self.drops = nn.Dropout(0.3)\n",
    "        #self.bn1 = nn.BatchNorm1d(d_in) #do batch normalizing transform\n",
    "        #self.bn2 = nn.BatchNorm1d(128)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #  3, 32, 32\n",
    "        # out_dim = in_dim - kernel_size + 1  \n",
    "        #32 - (5-1) -1 +1 =28\n",
    "        x = self.pool(F.relu(self.conv1(x))) #6, 132, 89 \n",
    "        x = self.pool(F.relu(self.conv2(x))) #16, 64, 43\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x))) \n",
    "        x = x.view(-1, 100 * 9 * 3)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x'''\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)  \n",
    "        # input channels = 3, 因為RGB所有3個channels, output為6個channels\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16,  kernel_size=5)  \n",
    "        self.fc1 = nn.Linear(16 * 64 * 42, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #  3, 32, 32\n",
    "        # out_dim = in_dim - kernel_size + 1  \n",
    "        #32 - (5-1) -1 +1 =28\n",
    "        x = self.pool(F.relu(self.conv1(x))) #6, 14, 14 \n",
    "        x = self.pool(F.relu(self.conv2(x))) #16, 5, 5\n",
    "        x = x.view(-1, 16 * 64 * 42)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 6, 264, 178]             456\n",
      "         MaxPool2d-2           [-1, 6, 132, 89]               0\n",
      "            Conv2d-3          [-1, 16, 128, 85]           2,416\n",
      "         MaxPool2d-4           [-1, 16, 64, 42]               0\n",
      "            Linear-5                  [-1, 120]       5,161,080\n",
      "            Linear-6                   [-1, 84]          10,164\n",
      "            Linear-7                    [-1, 7]             595\n",
      "================================================================\n",
      "Total params: 5,174,711\n",
      "Trainable params: 5,174,711\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.56\n",
      "Forward/backward pass size (MB): 4.35\n",
      "Params size (MB): 19.74\n",
      "Estimated Total Size (MB): 24.64\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, input_size=(3,268, 182))\n",
    "#summary(model, input_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(save_path, model, optimizer, val_loss):\n",
    "    if save_path==None:\n",
    "        return\n",
    "    save_path = save_path \n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'val_loss': val_loss}\n",
    "\n",
    "    torch.save(state_dict, save_path)\n",
    "\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "def load_checkpoint(model, optimizer):\n",
    "    save_path = f'cifar_net.pt'\n",
    "    state_dict = torch.load(save_path)\n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "    val_loss = state_dict['val_loss']\n",
    "    print(f'Model loaded from <== {save_path}')\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "\n",
    "\n",
    "def TRAIN(net, train_loader, valid_loader,  num_epochs, eval_every, total_step, criterion, optimizer, val_loss, device, save_name):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    running_num = 0\n",
    "    global_step = 0\n",
    "    if val_loss==None:\n",
    "        best_val_loss = float(\"Inf\")  \n",
    "    else: \n",
    "        best_val_loss=val_loss\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            net.train()\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            '''Training of the model'''\n",
    "            # Forward pass\n",
    "            outputs = net(inputs)\n",
    "            #_, preds = torch.max(outputs.data, 1)\n",
    "            for i in range(7):\n",
    "                if(outputs[0][i] < 0.5):\n",
    "                    outputs[0][i] = 0\n",
    "                else:\n",
    "                    outputs[0][i] = 1\n",
    "            preds = outputs\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            #if(torch.sum(preds == labels.data)):\n",
    "                #print('preds:', preds)\n",
    "                #print('labels.data:', labels.data)\n",
    "                #print('torch.sum(preds == labels.data):', torch.sum(preds == labels.data))\n",
    "            running_num += 7\n",
    "            #print(running_corrects)\n",
    "            #print(running_num)\n",
    "\n",
    "            '''Evaluating the model every x steps'''\n",
    "            if global_step % eval_every == 0:\n",
    "                with torch.no_grad():\n",
    "                    net.eval()\n",
    "                    val_running_loss = 0.0\n",
    "                    val_running_corrects = 0\n",
    "                    for val_inputs, val_labels in valid_loader: #do validation here\n",
    "                        val_outputs = net(val_inputs)\n",
    "                        val_loss = criterion(val_outputs, val_labels)\n",
    "                        _, preds = torch.max(val_outputs.data, 1)\n",
    "                        val_running_loss += val_loss.item()\n",
    "                        val_running_corrects += torch.sum(preds == val_labels.data)\n",
    "\n",
    "                    average_train_loss = running_loss / eval_every\n",
    "                    average_val_loss = val_running_loss / len(valid_loader)\n",
    "                    print('running_corrects: ', running_corrects)\n",
    "                    print('running_num:',  float(running_num))\n",
    "                    average_train_acc = running_corrects / float(running_num)\n",
    "                    average_val_acc = val_running_corrects / float(len(valid_loader))\n",
    "\n",
    "                    print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Train Acc: {:.4f}, Valid Loss: {:.4f},  Valid Acc: {:.4f}'\n",
    "                          .format(epoch+1, num_epochs, global_step, total_step, average_train_loss,\n",
    "                                  average_train_acc, average_val_loss, average_val_acc))\n",
    "\n",
    "                    running_loss = 0.0\n",
    "                    running_num = 0\n",
    "                    running_corrects = 0\n",
    "                    \n",
    "                    if average_val_loss < best_val_loss:\n",
    "                        best_val_loss = average_val_loss\n",
    "                        save_checkpoint(save_name, net, optimizer, best_val_loss)#save the best model\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running_corrects:  tensor(5265)\n",
      "running_num: 7000.0\n",
      "Epoch [1/1], Step [1000/8132], Train Loss: 0.6931, Train Acc: 0.7521, Valid Loss: 0.6907,  Valid Acc: 1.7127\n",
      "Model saved to ==> cifar_net.pt\n",
      "running_corrects:  tensor(5270)\n",
      "running_num: 7000.0\n",
      "Epoch [1/1], Step [2000/8132], Train Loss: 0.6931, Train Acc: 0.7529, Valid Loss: 0.6907,  Valid Acc: 1.7127\n",
      "running_corrects:  tensor(5271)\n",
      "running_num: 7000.0\n",
      "Epoch [1/1], Step [3000/8132], Train Loss: 0.6931, Train Acc: 0.7530, Valid Loss: 0.6907,  Valid Acc: 1.7127\n",
      "running_corrects:  tensor(5264)\n",
      "running_num: 7000.0\n",
      "Epoch [1/1], Step [4000/8132], Train Loss: 0.6931, Train Acc: 0.7520, Valid Loss: 0.6907,  Valid Acc: 1.7127\n",
      "running_corrects:  tensor(5282)\n",
      "running_num: 7000.0\n",
      "Epoch [1/1], Step [5000/8132], Train Loss: 0.6931, Train Acc: 0.7546, Valid Loss: 0.6907,  Valid Acc: 1.7127\n",
      "running_corrects:  tensor(5262)\n",
      "running_num: 7000.0\n",
      "Epoch [1/1], Step [6000/8132], Train Loss: 0.6931, Train Acc: 0.7517, Valid Loss: 0.6907,  Valid Acc: 1.7127\n",
      "running_corrects:  tensor(5259)\n",
      "running_num: 7000.0\n",
      "Epoch [1/1], Step [7000/8132], Train Loss: 0.6931, Train Acc: 0.7513, Valid Loss: 0.6907,  Valid Acc: 1.7127\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "eval_every = 1000\n",
    "total_step = len(train_loader)*num_epochs\n",
    "best_val_loss = None\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "save_path = f'cifar_net.pt'\n",
    "model = model.to(device)\n",
    "\n",
    "TRAIN(model, train_loader, valid_loader, num_epochs, eval_every, total_step, criterion, optimizer, best_val_loss, device, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0950, 0.5425, 0.2409], grad_fn=<SigmoidBackward>)\n",
      "tensor([1., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Sigmoid()\n",
    "loss = nn.BCELoss()\n",
    "input = torch.randn(3, requires_grad=True)\n",
    "target = torch.empty(3).random_(2)\n",
    "print(m(input))\n",
    "print(target)\n",
    "output = loss(m(input), target)\n",
    "output.backward()\n",
    "\n",
    "a = np.zeros(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asdasd\n"
     ]
    }
   ],
   "source": [
    "print('asd'+'asd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
